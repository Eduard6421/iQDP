{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc0888a-77a2-4998-9194-231d562aff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------- Imports --------------------------------------------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import skimage.io as sk\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.transforms import Resize,Compose, ToTensor\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.svm import LinearSVR\n",
    "from torchvision.models import vgg16,VGG16_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7165ae-e2cc-4da1-ab89-43fc426ab63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c6491ac15f439aaea8b1af9e52df65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "%run /notebooks/Referenced-Models/ionescu-et-all.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b0e9f-a60a-46ed-a7a7-b761b62d85cf",
   "metadata": {},
   "source": [
    "# Ionescu et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce4f3e80-a776-476e-9e91-9f22456f5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGExtractor():\n",
    "    \n",
    "    def __init__(self, ):\n",
    "    \n",
    "        # Pytorch exposes easy preprocess steps so that we can match the input data\n",
    "        self.vgg16_model = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "\n",
    "        # Remove last linear and keep only 4096d vectors\n",
    "        self.vgg16_model.classifier = self.vgg16_model.classifier[:-1]\n",
    "        self.vgg16_model.to(device)\n",
    "    \n",
    "        for parameter in self.vgg16_model.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        \n",
    "        self.vgg16_model.eval()\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        \n",
    "        return self.vgg16_model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011f506b-0e32-4d1f-bd85-015f4ecc6853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "extractor = VGGExtractor()\n",
    "scaler = load('/notebooks/Referenced-Models/scaler.joblib')\n",
    "svr  = load('/notebooks/Referenced-Models/svr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fcd726b-8c18-462f-8bc6-e0f43e65df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GND_PATH = \"/notebooks/cnnimageretrieval-pytorch/data/test/caltech101/gnd_caltech101_700.pkl\"\n",
    "DATASET_FOLDERPATH = '/notebooks/cnnimageretrieval-pytorch/data/test/caltech101/jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160298a1-5d6f-4957-ac5d-143c7c053a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(GND_PATH, 'rb') as handle:\n",
    "    gnd_file = pickle.load(handle)\n",
    "    \n",
    "query_images = gnd_file['qimlist']\n",
    "details = gnd_file['gnd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6241e51e-c70b-4254-92d2-4e563ef5fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['path','score'])\n",
    "\n",
    "transform_fct = VGG16_Weights.DEFAULT.transforms() \n",
    "to_tensor = ToTensor()\n",
    "\n",
    "\n",
    "for i in range(len(query_images)):\n",
    "    image_path = os.path.join(DATASET_FOLDERPATH,query_images[i]) + '.jpg'\n",
    "    image = Image.open(image_path)\n",
    "    image = to_tensor(image)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    if(image.shape[0] == 1):\n",
    "        image = torch.cat([image,image,image],axis = 0)\n",
    "        \n",
    "    if(gnd_file['gnd'][i]['bbx'] is None or None in gnd_file['gnd'][i]['bbx']):\n",
    "        crop = image\n",
    "    else:\n",
    "        bbox = gnd_file['gnd'][i]['bbx']\n",
    "        [xmin,ymin,xmax,ymax] = bbox\n",
    "        xmin = int(xmin)\n",
    "        ymin = int(ymin)\n",
    "        xmax = int(xmax)\n",
    "        ymax = int(ymax)\n",
    "        crop = image[:,ymin:ymax,xmin:xmax]\n",
    "        \n",
    "    #print(details[0])\n",
    "    #plt.imshow(crop.cpu().permute(1,2,0))\n",
    "    #raise Exception('asd')\n",
    "\n",
    "    crop = transform_fct(crop)\n",
    "    \n",
    "    crop = crop.unsqueeze(0)\n",
    "    \n",
    "    model_output = model(crop)\n",
    "    \n",
    "    \n",
    "    scaled_features = scaler.transform(model_output.cpu().numpy())\n",
    "    \n",
    "    score = svr.predict(scaled_features)[0]\n",
    "    \n",
    "    new_df = pd.DataFrame([{\n",
    "        'path' : image_path,\n",
    "        'score': score\n",
    "    }])\n",
    "    df = pd.concat([df, new_df], axis=0, ignore_index=True)\n",
    "        #query_image = qimlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12c1995-02d0-4d9d-a358-a4487b42abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/notebooks/Results/ionescu-et-al-caltech101_700.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b03fd5-63c8-4bba-b824-52f8ec13cc44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13042e-fc5b-4cc2-a687-4006ab9fc10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7df7a8-957c-478b-9e39-8c6405d34b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
