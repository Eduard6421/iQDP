{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76768609-7211-4d04-a91d-a3a8ed0a52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "from torch.nn import Sequential\n",
    "import warnings\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "import skimage.io as sk\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa771446-05b6-45a1-9f49-673273f5f39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running on cuda\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------- Global settings --------------------------------------------------------------\n",
    "warnings.simplefilter(\"always\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Currently running on {device}\".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbfec3f-86e6-44c4-8ca0-5522baee47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectnessDifficultyRegressor():\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        \n",
    "        self.weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "        \n",
    "        self.content_transform = FasterRCNN_ResNet50_FPN_Weights.COCO_V1.transforms()\n",
    "        self.content_transform = Sequential(Resize((224,224)),self.content_transform)\n",
    "\n",
    "        self.model = fasterrcnn_resnet50_fpn(weights = self.weights, progress=False, trainable_backbone_layers=0)\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "        \n",
    "    \n",
    "    def __call__(self, image):\n",
    "\n",
    "        image = image.to(device)\n",
    "        resized_image = self.content_transform(image)\n",
    "        result = self.model([resized_image])\n",
    "        boxes = result[0]['boxes']\n",
    "        boxes = boxes.tolist()        \n",
    "        n = 0\n",
    "        total_area = 0\n",
    "    \n",
    "        for x1,y1,x2,y2 in boxes:\n",
    "            width  = x2 - x1 \n",
    "            height = y2 - y1\n",
    "            area = width * height\n",
    "            n = n + 1\n",
    "            total_area += area\n",
    "        \n",
    "        if(n == 0):\n",
    "            return 9999999999999\n",
    "        else:\n",
    "            return  n * n / total_area\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5efb5c8d63a54c73d8d27071022adad53c6f3df0ecaf8b2d6fe4415ee6c014ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
