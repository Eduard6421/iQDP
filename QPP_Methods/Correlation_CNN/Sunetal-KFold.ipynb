{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ff0186-1643-4009-bc11-0f3fdc9945f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset,DataLoader,random_split\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd11c7a-56ab-44cc-b86f-9ae6d5b5b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running on cuda\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------- Global settings --------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Currently running on {device}\".format())\n",
    "\n",
    "BATCH_SIZE = 70\n",
    "NUM_EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fdf55f0-d5a4-430a-9b42-916a5e339f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_emb = '/../../Embeddings/DEEP_Image_Retrieval/rparis6k-top-100-results-and-scores.csv'\n",
    "scores_path = '/../../Results/deepretrieval-rparis6k-ap.csv'\n",
    "FOLD_PATH = '/../../Folds/rparis6k-folds.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c530df-c55c-4cd4-8314-f8eafb4d697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_csv(top_100_emb)\n",
    "scores_df = pd.read_csv(scores_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e3e2d1f-8b2e-4025-b639-2df6b17bf164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x,y):\n",
    "    return dot(x,y) / (norm(x) * norm(y))\n",
    "\n",
    "def parse_input(content): \n",
    "    regex = r\"\\[([0-9\\s,\\-\\.e]+)\\]\"\n",
    "    items = re.findall(regex, content)\n",
    "    parsed_input = np.array([np.fromstring(embed, sep=',') for embed in items]).astype(float)\n",
    "    return parsed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c09ec9-de41-4e1b-9474-33ba88ce007b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "for query_idx in range(len(embeddings_df)):\n",
    "    if(query_idx % 50 == 0):\n",
    "        print(query_idx)\n",
    "    image_embeddings = embeddings_df['result_emb'].iloc[query_idx]\n",
    "    image_embeddings = image_embeddings[1:-1]\n",
    "    image_embeddings = parse_input(image_embeddings)\n",
    "    inputs.append(image_embeddings)\n",
    "    \n",
    "inputs = np.array(inputs)\n",
    "scores = scores_df['score'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9611d6e9-a496-4cbf-b788-3e8c2accbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_maps = []\n",
    "for query_idx in range(len(inputs)):\n",
    "    result_elements = inputs[query_idx,:,:]\n",
    "    similarity_matrix = np.zeros((100,100))\n",
    "    for i in range(100):\n",
    "        for j in range(i, 100):\n",
    "            similarity_matrix[i][j] = similarity_matrix[j][i] = cosine_similarity(result_elements[i],result_elements[j])\n",
    "    input_maps.append(similarity_matrix)\n",
    "input_maps = np.array(input_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "934411f4-eb08-453c-a705-89bdba9e90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_maps = input_maps.reshape(len(inputs),1,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8651d53e-d6d2-4122-8bd0-ab7535ecff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = scores_df[['path']].to_numpy().squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b0f7363-6fc0-45b8-84e1-8712ad3a9428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_461/211984777.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dataset = np.array(list(zip(input_maps,scores, paths)))\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array(list(zip(input_maps,scores, paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b112782-5478-4023-a2bd-f4c68089042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_file = open(FOLD_PATH, 'rb')\n",
    "folds = pickle.load(fold_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be3222d2-c159-4180-8ab3-3903fa32721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifficultyFoldDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.correlation_matrices = data[:,0]\n",
    "        self.scores = data[:,1]\n",
    "        self.paths  = data[:,2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scores)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        matrix = torch.tensor(self.correlation_matrices[idx])\n",
    "        score = torch.tensor(float(self.scores[idx]))\n",
    "        query_path = self.paths[idx]\n",
    "\n",
    "        return (matrix, score, query_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "715eb3c0-99da-4f97-b3f4-c0498a9d827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model , loader):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, scores, paths = data\n",
    "            \n",
    "            images = images.to(device,dtype=torch.float)\n",
    "            scores = scores.to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, scores)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    total_loss /= len(loader)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f46f6750-832b-4a3f-a649-5ce2cca2ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, train_dataloader, test_dataloader, optimizer , criterion):\n",
    "    \n",
    "    min_loss = 1000\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        \n",
    "        epoch_train_loss = 0\n",
    "        \n",
    "        for idx, data in enumerate(train_dataloader):\n",
    "            #print(\"Batch num {}/{}\".format(idx+1, len(train_dataloader)))\n",
    " \n",
    "            (images,scores,img_paths) = data\n",
    "    \n",
    "            images = images.to(device,dtype=torch.float)\n",
    "        \n",
    "            scores = scores.to(device).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(scores, outputs)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "        epoch_test_loss = compute_loss(model, test_dataloader)\n",
    "\n",
    "        epoch_train_loss /= len(train_dataloader)\n",
    "        if((i+1)%50== 0):\n",
    "            print(\"Epoch num {}/{}\".format(i+1,NUM_EPOCHS))\n",
    "            print(\"Epoch train loss {}\".format(epoch_train_loss))\n",
    "            print(\"Epoch test loss {}\".format(epoch_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "311ca731-e29f-4250-8cb3-b2abf0d61d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20 , 3, stride = 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50 , 3, stride = 1)\n",
    "        self.conv3 = nn.Conv2d(50, 50 , 3, stride = 1)\n",
    "        self.fc1   = nn.Linear(5000, 256)\n",
    "        self.fc2   = nn.Linear(256, 1)\n",
    "        \n",
    "        self.max_pool   = nn.MaxPool2d(2)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.leaky_relu(x)        \n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        \n",
    "        x = x.reshape(-1,5000)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50c9dc29-c051-4825-a9d6-c6ed8104d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch num 50/500\n",
      "Epoch train loss 0.01168735045939684\n",
      "Epoch test loss 0.02569313533604145\n",
      "Epoch num 100/500\n",
      "Epoch train loss 0.002131641609594226\n",
      "Epoch test loss 0.017728669568896294\n",
      "Epoch num 150/500\n",
      "Epoch train loss 0.0006716835196129978\n",
      "Epoch test loss 0.01785700023174286\n",
      "Epoch num 200/500\n",
      "Epoch train loss 0.0003519585297908634\n",
      "Epoch test loss 0.0172664076089859\n",
      "Epoch num 250/500\n",
      "Epoch train loss 0.00021441237186081707\n",
      "Epoch test loss 0.016955211758613586\n",
      "Epoch num 300/500\n",
      "Epoch train loss 9.875724936136976e-05\n",
      "Epoch test loss 0.01751518063247204\n",
      "Epoch num 350/500\n",
      "Epoch train loss 0.000842677429318428\n",
      "Epoch test loss 0.018488997593522072\n",
      "Epoch num 400/500\n",
      "Epoch train loss 3.446583286859095e-05\n",
      "Epoch test loss 0.01793629489839077\n",
      "Epoch num 450/500\n",
      "Epoch train loss 1.3643276361108292e-05\n",
      "Epoch test loss 0.017857922241091728\n",
      "Epoch num 500/500\n",
      "Epoch train loss 6.405156909750076e-06\n",
      "Epoch test loss 0.017890751361846924\n",
      "Epoch num 50/500\n",
      "Epoch train loss 0.019208258017897606\n",
      "Epoch test loss 0.029767930507659912\n",
      "Epoch num 100/500\n",
      "Epoch train loss 0.012083531357347965\n",
      "Epoch test loss 0.0231966320425272\n",
      "Epoch num 150/500\n",
      "Epoch train loss 0.005564267281442881\n",
      "Epoch test loss 0.019055355340242386\n",
      "Epoch num 200/500\n",
      "Epoch train loss 0.001999825704842806\n",
      "Epoch test loss 0.019904401153326035\n",
      "Epoch num 250/500\n",
      "Epoch train loss 0.001837838557548821\n",
      "Epoch test loss 0.022469080984592438\n",
      "Epoch num 300/500\n",
      "Epoch train loss 0.000579803017899394\n",
      "Epoch test loss 0.019439605996012688\n",
      "Epoch num 350/500\n",
      "Epoch train loss 0.00032156973611563444\n",
      "Epoch test loss 0.019223501905798912\n",
      "Epoch num 400/500\n",
      "Epoch train loss 0.00017770699923858047\n",
      "Epoch test loss 0.01857745088636875\n",
      "Epoch num 450/500\n",
      "Epoch train loss 0.000206884607905522\n",
      "Epoch test loss 0.019152328372001648\n",
      "Epoch num 500/500\n",
      "Epoch train loss 0.00010957755876006559\n",
      "Epoch test loss 0.01925431191921234\n",
      "Epoch num 50/500\n",
      "Epoch train loss 0.018969932571053505\n",
      "Epoch test loss 0.008508225902915001\n",
      "Epoch num 100/500\n",
      "Epoch train loss 0.004987348802387714\n",
      "Epoch test loss 0.010793925262987614\n",
      "Epoch num 150/500\n",
      "Epoch train loss 0.0013759232824668288\n",
      "Epoch test loss 0.005985063500702381\n",
      "Epoch num 200/500\n",
      "Epoch train loss 0.00041658474947325885\n",
      "Epoch test loss 0.006317853461951017\n",
      "Epoch num 250/500\n",
      "Epoch train loss 0.0003670099831651896\n",
      "Epoch test loss 0.007150635588914156\n",
      "Epoch num 300/500\n",
      "Epoch train loss 0.00010081609798362479\n",
      "Epoch test loss 0.006208149250596762\n",
      "Epoch num 350/500\n",
      "Epoch train loss 5.1356528274482116e-05\n",
      "Epoch test loss 0.005682582035660744\n",
      "Epoch num 400/500\n",
      "Epoch train loss 0.0006424924358725548\n",
      "Epoch test loss 0.005755317397415638\n",
      "Epoch num 450/500\n",
      "Epoch train loss 1.2849292033934034e-05\n",
      "Epoch test loss 0.0060351998545229435\n",
      "Epoch num 500/500\n",
      "Epoch train loss 4.001896741101518e-06\n",
      "Epoch test loss 0.005889394320547581\n",
      "Epoch num 50/500\n",
      "Epoch train loss 0.015551593154668808\n",
      "Epoch test loss 0.022396432235836983\n",
      "Epoch num 100/500\n",
      "Epoch train loss 0.007056731730699539\n",
      "Epoch test loss 0.0295465886592865\n",
      "Epoch num 150/500\n",
      "Epoch train loss 0.002063324209302664\n",
      "Epoch test loss 0.03445068374276161\n",
      "Epoch num 200/500\n",
      "Epoch train loss 0.0007618165109306574\n",
      "Epoch test loss 0.03679005056619644\n",
      "Epoch num 250/500\n",
      "Epoch train loss 0.0002662964107003063\n",
      "Epoch test loss 0.036258500069379807\n",
      "Epoch num 300/500\n",
      "Epoch train loss 0.00015246409748215228\n",
      "Epoch test loss 0.03522109240293503\n",
      "Epoch num 350/500\n",
      "Epoch train loss 6.536552973557264e-05\n",
      "Epoch test loss 0.03578206151723862\n",
      "Epoch num 400/500\n",
      "Epoch train loss 0.00023805211822036654\n",
      "Epoch test loss 0.035739716142416\n",
      "Epoch num 450/500\n",
      "Epoch train loss 2.0973533537471667e-05\n",
      "Epoch test loss 0.03580842912197113\n",
      "Epoch num 500/500\n",
      "Epoch train loss 9.346365004603285e-06\n",
      "Epoch test loss 0.03568027913570404\n",
      "Epoch num 50/500\n",
      "Epoch train loss 0.02004760503768921\n",
      "Epoch test loss 0.017010603100061417\n",
      "Epoch num 100/500\n",
      "Epoch train loss 0.005251229275017977\n",
      "Epoch test loss 0.04085154831409454\n",
      "Epoch num 150/500\n",
      "Epoch train loss 0.0026345201767981052\n",
      "Epoch test loss 0.06476247310638428\n",
      "Epoch num 200/500\n",
      "Epoch train loss 0.00048567389603704214\n",
      "Epoch test loss 0.06158677116036415\n",
      "Epoch num 250/500\n",
      "Epoch train loss 0.000544559268746525\n",
      "Epoch test loss 0.05294455960392952\n",
      "Epoch num 300/500\n",
      "Epoch train loss 0.0002775837783701718\n",
      "Epoch test loss 0.06353982537984848\n",
      "Epoch num 350/500\n",
      "Epoch train loss 0.00021946773631498218\n",
      "Epoch test loss 0.05497794598340988\n",
      "Epoch num 400/500\n",
      "Epoch train loss 9.104961645789444e-05\n",
      "Epoch test loss 0.05742933228611946\n",
      "Epoch num 450/500\n",
      "Epoch train loss 3.872826346196234e-05\n",
      "Epoch test loss 0.05249942094087601\n",
      "Epoch num 500/500\n",
      "Epoch train loss 2.2398377041099593e-05\n",
      "Epoch test loss 0.05262252315878868\n"
     ]
    }
   ],
   "source": [
    "score_dict = {}\n",
    "for i, (train_index, test_index) in enumerate(folds):\n",
    "    train_data = np.array(dataset[train_index])\n",
    "    test_data  = np.array(dataset[test_index])\n",
    "    \n",
    "    train_dataset = DifficultyFoldDataset(train_data)\n",
    "    test_dataset  = DifficultyFoldDataset(test_data)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    cnn_model = CNN_Network()\n",
    "    \n",
    "    cnn_model = cnn_model.to(device)  \n",
    "    cnn_model.train()\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = Adam(cnn_model.parameters(), lr=0.0005)\n",
    "    train_model(cnn_model, train_dataloader, test_dataloader, optimizer, criterion)\n",
    "    cnn_model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    for item in test_dataset:\n",
    "        image, true_score, path = item\n",
    "        score = cnn_model(image.unsqueeze(0).to(device,dtype=torch.float))\n",
    "        score_dict[path] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f641710b-146f-4c5d-85a2-1d8ca6cf899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for path in paths:\n",
    "    scores.append(float(score_dict[path].detach().cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "167e08df-23d2-467b-8c0d-42d5d7703b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({'path': paths, 'score': scores})\n",
    "result_df.to_csv('/../../Results/sunetal-deepretrieval-rparis6k-ap.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54abf6-8b42-4db7-bda4-4f46c30384cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3dbe7-6247-49f5-b7aa-9e2f50c8faee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b01da-65c6-4eec-9291-261d1e9559e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae91af2-5319-4b5e-876d-b0419b09116d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb1d1f-fa2e-4861-99f2-92ca92c201dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053960f1-8686-4f85-91b2-9ebbdc13793e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
